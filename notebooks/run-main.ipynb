{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import transformers\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch.nn as nn\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport numpy as np\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nimport torch.nn as nn\nimport torch\n\n\nMAX_LEN = 512\nTRAINING_BATCH_SIZE = 16\nVALIDATION_BATCH_SIZE = 8\nEPOCHS = 2\nTRAINING_PATH = '../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'\nBERT_PATH = '../input/bert-base-uncased/'\nMODEL_PATH = 'model.bin'\n# Bert tokenizer is WorldPiece tokenizer\nTOKENIZER = transformers.BertTokenizer.from_pretrained(\n    pretrained_model_name_or_path= BERT_PATH,\n    do_lower_case = True\n)\n\n","execution_count":1,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERTsentiment(nn.Module):\n    def __init__(self):\n        super(BERTsentiment, self).__init__()\n        \n        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n        self.drop = nn.Dropout(0.5) # for regularization\n        self.out_layer = nn.Linear(768, 1) # BERT model uses 768 in last, 1 output because its binary \n        \n        \n    def forward(self, input_ids, attention_mask, token_type_ids):\n        # out1 = Sequence of hidden states at the output of the last layer of the model\n        # out2 =  Last layer hidden-state of the first token of the sequence (classification token) further processed by a Linear layer and a Tanh activation function.\n        # The Linear layer weights are trained from the next sentence prediction (classification) objective during pretraining.\n        out1, out2 = self.bert(\n            input_ids = input_ids, # token indices \n            attention_mask = attention_mask, # indices for padding 0 and 1\n            token_type_ids = token_type_ids  # indices for sentences, we dont really need this becasue our input is only one sentence so its always gona be 0\n        )\n        \n        bert_output = self.drop(out2) # apply Dropout\n        output = self.out_layer(bert_output) # pass to Linear layer\n        \n        return output # Linear output \n    ","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\nclass BERTdataset:\n    def __init__(self, review, sentiment):\n        self.review = review # input\n        self.sentiment = sentiment # target\n        self.tokenizer = TOKENIZER\n        self.max_length = MAX_LEN\n        \n        \n    def __len__(self):\n        return len(self.review)\n    \n    \n    \n    def __getitem__(self, item_index):\n        review = str(self.review[item_index])\n        review = ' '.join(review.split()) # first make a list out of sentences than make a sentnces with only one space between words\n                                          #this just removes if there are some weired spaces between words\n            \n        # BERT can take as input either one or two sentences, and uses [SEP] token to separate them.\n        # [CLS] token always appears at start of sentences\n        # Both tokens are always required even if we only have one sentences becasue thats how BERT was pretrained and how expects input\n        inputs = self.tokenizer.encode_plus(\n            review,\n            None,\n            add_special_tokens = True,\n            max_length = self.max_length,\n            truncation = True\n        )\n        \n        input_ids = inputs['input_ids']\n        attention_mask = inputs['attention_mask']\n        token_type_ids = inputs['token_type_ids']\n        \n        # we could have done padding  as parametar in encode_plus but lets act fancy\n        padding_length = self.max_length - len(input_ids)\n        \n        input_ids = input_ids + ([0] * padding_length)             # add [0] to the max lenght\n        attention_mask = attention_mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        \n        # return tensors\n        return {\n            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'sentiments': torch.tensor(self.sentiment[item_index], dtype=torch.float)\n        }\n    ","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef loss_function(outputs, sentiments):\n    return nn.BCEWithLogitsLoss()(outputs,sentiments.view(-1, 1))\n    # This loss combines a Sigmoid layer and the BCELoss in one single class.\n\n\n\n\ndef training_loop(training_data_loader, model, optimizer, scheduler, device):\n    # training state\n    model.train()\n    \n    for batch_index, dataset in tqdm(enumerate(training_data_loader), total=len(training_data_loader)):\n        # load from dataset\n        input_ids = dataset['input_ids']\n        attention_mask = dataset['attention_mask']\n        token_type_ids = dataset['token_type_ids']\n        sentiments = dataset['sentiments']\n        # move to cuda device\n        input_ids = input_ids.to(device, dtype=torch.long)\n        attention_mask = attention_mask.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        sentiments = sentiments.to(device, dtype=torch.float)\n        \n        # set gradients to zero before every backprop becasue pytorch does not do that\n        optimizer.zero_grad()\n        outputs = model(\n            input_ids = input_ids,\n            attention_mask = attention_mask,\n            token_type_ids = token_type_ids,\n        )\n        \n        loss = loss_function(outputs, sentiments)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        if batch_index % 500 == 0 and batch_index != 0:\n            print('BATCH_INDEX: ',batch_index ,'==========', 'LOSS: ', loss.item())\n            \n            \ndef evaluation_loop(validation_data_loader, model, device):\n    \n    # evaluation state\n    model.eval()\n    final_sentiments = []\n    final_outputs = []\n    with torch.no_grad():\n        # deactivate autograd, helps with memory usage\n        \n        for batch_index, dataset in tqdm(enumerate(validation_data_loader), total=len(validation_data_loader)):\n            # load from dataset\n            input_ids = dataset['input_ids']\n            attention_mask = dataset['attention_mask']\n            token_type_ids = dataset['token_type_ids']\n            sentiments = dataset['sentiments']\n            # move to cuda device\n            input_ids = input_ids.to(device, dtype=torch.long)\n            attention_mask = attention_mask.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            sentiments = sentiments.to(device, dtype=torch.float)\n\n            # set gradients to zero before every backprop becasue pytorch does not do that\n            outputs = model(\n                input_ids = input_ids,\n                attention_mask = attention_mask,\n                token_type_ids = token_type_ids,\n            )\n            \n            final_sentiments.extend(sentiments.cpu().detach().numpy().tolist())\n            # move to cpu\n            # detach beacause no need for gradients\n            # numpy array\n            #list\n            final_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n            \n    return final_outputs, final_sentiments    \n        ","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n    \ndataframe = pd.read_csv(TRAINING_PATH) # load dataframe\ndataframe.sentiment = dataframe.sentiment.apply(\n        lambda x: 1 if x == 'positive' else 0\n    )\n    # sentiment is category target variable so we have to label encode it, we can do it like this by hand, or simply with sklearn.model_selection.LabelEncoder\n    \n    \n    # now split data into validation and training\n\ndf_train, df_valid = model_selection.train_test_split(\n        dataframe,\n        test_size = 0.1, # 10 percent of dataframe will be for validation\n        random_state = 42, # if we are going to run multiple time this script, random state enables that everytime we get same split with same random state\n        shuffle = True, # shuffle indices\n        stratify = dataframe.sentiment.values # same distribution in train and valid \n    )\n    \ndf_train = df_train.reset_index(drop=True) # we reset indices from 0 to len(df_train)\ndf_valid = df_valid.reset_index(drop=True) # we reset indices from 0 to len(df_valid)\n    \n    # make datasets with our class in order to make data loaders\ntraining_dataset = BERTdataset(\n        review = df_train.review.values,\n        sentiment = df_train.sentiment.values\n    )\n    # from dataset to dataloader\ntraining_data_loader = torch.utils.data.DataLoader(\n        dataset = training_dataset,\n        batch_size = TRAINING_BATCH_SIZE,\n        shuffle = True,\n        num_workers = 4\n    )\n    \nvalidation_dataset = BERTdataset(\n        review = df_valid.review.values,\n        sentiment = df_valid.sentiment.values,\n    )\n    # from dataset to dataloader\nvalidation_data_loader = torch.utils.data.DataLoader(\n        dataset = validation_dataset,\n        batch_size = VALIDATION_BATCH_SIZE,\n        shuffle = False,\n        num_workers = 4\n    )\n    \ndevice = torch.device('cuda')\nmodel = BERTsentiment()\nmodel.to(device) # move model to cuda device \n    # params to optimize \nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if  any(nd in n for nd in no_decay)], 'weight_decay': 0.00}\n    ]\n    \nnumber_of_training_steps = int(len(df_train) / TRAINING_BATCH_SIZE * EPOCHS) \n    #AdamW focuses on regularization and model does better on  generalization\noptimizer = AdamW(\n        params = optimizer_parameters,\n        lr = 3e-5\n    )\nscheduler = get_linear_schedule_with_warmup(\n        optimizer = optimizer,\n        num_warmup_steps = 0,\n        num_training_steps = number_of_training_steps,\n        \n    )","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_accuracy = []\n    \nfor epoch in range(EPOCHS):\n    print('EPOCH:', epoch + 1)\n    training_loop(\n            training_data_loader,\n            model,\n            optimizer,\n            scheduler,\n            device)\n    outputs, sentiments = evaluation_loop(\n            validation_data_loader, \n            model, \n            device)\n        # distribution is 50 50 so we can use acc score\n    outputs = np.array(outputs) >= 0.5 # positive class\n    accuracy = metrics.accuracy_score(sentiments, outputs)\n    print('ACCURACY SCORE',{accuracy})\n        \n    if accuracy > best_accuracy:\n        torch.save(model.state_dict(),MODEL_PATH) # save model in working dir\n        best_accuracy = accuracy\n","execution_count":6,"outputs":[{"output_type":"stream","text":"EPOCH: 1\n","name":"stdout"},{"output_type":"stream","text":" 18%|█▊        | 501/2813 [07:07<32:38,  1.18it/s]","name":"stderr"},{"output_type":"stream","text":"BATCH_INDEX:  500 ========== LOSS:  0.050576530396938324\n","name":"stdout"},{"output_type":"stream","text":" 36%|███▌      | 1001/2813 [14:12<25:42,  1.18it/s]","name":"stderr"},{"output_type":"stream","text":"BATCH_INDEX:  1000 ========== LOSS:  0.33655062317848206\n","name":"stdout"},{"output_type":"stream","text":" 53%|█████▎    | 1501/2813 [21:17<18:42,  1.17it/s]","name":"stderr"},{"output_type":"stream","text":"BATCH_INDEX:  1500 ========== LOSS:  0.019793197512626648\n","name":"stdout"},{"output_type":"stream","text":" 71%|███████   | 2001/2813 [28:22<11:34,  1.17it/s]","name":"stderr"},{"output_type":"stream","text":"BATCH_INDEX:  2000 ========== LOSS:  0.049047671258449554\n","name":"stdout"},{"output_type":"stream","text":" 89%|████████▉ | 2501/2813 [35:28<04:25,  1.18it/s]","name":"stderr"},{"output_type":"stream","text":"BATCH_INDEX:  2500 ========== LOSS:  0.058641016483306885\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 2813/2813 [39:53<00:00,  1.18it/s]\n100%|██████████| 625/625 [01:37<00:00,  6.41it/s]","name":"stderr"},{"output_type":"stream","text":"ACCURACY SCORE {0.937}\nEPOCH: 2\n","name":"stdout"},{"output_type":"stream","text":"\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n 18%|█▊        | 501/2813 [07:06<32:40,  1.18it/s]","name":"stderr"},{"output_type":"stream","text":"BATCH_INDEX:  500 ========== LOSS:  0.2697891294956207\n","name":"stdout"},{"output_type":"stream","text":" 36%|███▌      | 1001/2813 [14:11<25:37,  1.18it/s]","name":"stderr"},{"output_type":"stream","text":"BATCH_INDEX:  1000 ========== LOSS:  0.3758668005466461\n","name":"stdout"},{"output_type":"stream","text":" 53%|█████▎    | 1501/2813 [21:17<18:38,  1.17it/s]","name":"stderr"},{"output_type":"stream","text":"BATCH_INDEX:  1500 ========== LOSS:  0.13920484483242035\n","name":"stdout"},{"output_type":"stream","text":" 71%|███████   | 2001/2813 [28:21<11:35,  1.17it/s]","name":"stderr"},{"output_type":"stream","text":"BATCH_INDEX:  2000 ========== LOSS:  0.020142626017332077\n","name":"stdout"},{"output_type":"stream","text":" 89%|████████▉ | 2501/2813 [35:27<04:23,  1.18it/s]","name":"stderr"},{"output_type":"stream","text":"BATCH_INDEX:  2500 ========== LOSS:  0.056109026074409485\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 2813/2813 [39:52<00:00,  1.18it/s]\n100%|██████████| 625/625 [01:36<00:00,  6.45it/s]","name":"stderr"},{"output_type":"stream","text":"ACCURACY SCORE {0.9504}\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}